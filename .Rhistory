user.view
sim.items <- cor.matrix[3,]
sim.items.sorted[1:5]
sim.items.sorted <- sort(sim.items, decreasing = TRUE)
sim.items.sorted[1:5]
library(twitteR)
install.packages("twitterR")
api_key = "'yYf0zjyxgZqlu9j5MDOWq6okG'"
api_secret = "OQtmCCacdDMJPNDgxhOFNy69LJEhIzzCRKQvn0HnsK7f8wRcLp"
access_token = "328069252-DNksJDI286YcorHMRYDXMlNpo0nWFh4qwA4mds91"
access_token_secret = "hUSBB0N9xzbpJ6Y8o2BRYVCVf3YXcG6P71BUljKSjUlmO"
library(twitteR)
install.packages("twitterR")
install.packages('twitterR', dependencies=TRUE, repos='http://cran.rstudio.com/')
library(twitteR)
install.packages("twitteR")
library(twitteR)
api_key = "'yYf0zjyxgZqlu9j5MDOWq6okG'"
api_secret = "OQtmCCacdDMJPNDgxhOFNy69LJEhIzzCRKQvn0HnsK7f8wRcLp"
access_token = "328069252-DNksJDI286YcorHMRYDXMlNpo0nWFh4qwA4mds91"
access_token_secret = "hUSBB0N9xzbpJ6Y8o2BRYVCVf3YXcG6P71BUljKSjUlmO"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
rent = searchTwitter("rent_in_ireland OR #rent_in_ireland OR accommodation_in_ireland OR
#accommodation_in_ireland OR housing_in_ireland OR #housing_in_ireland", n = 3000, lang = 'en')
library(twitteR)
api_key = "'yYf0zjyxgZqlu9j5MDOWq6okG'"
api_secret = "OQtmCCacdDMJPNDgxhOFNy69LJEhIzzCRKQvn0HnsK7f8wRcLp"
access_token = "328069252-DNksJDI286YcorHMRYDXMlNpo0nWFh4qwA4mds91"
access_token_secret = "hUSBB0N9xzbpJ6Y8o2BRYVCVf3YXcG6P71BUljKSjUlmO"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
library(twitteR)
api_key = "'yYf0zjyxgZqlu9j5MDOWq6okG'"
api_secret = "OQtmCCacdDMJPNDgxhOFNy69LJEhIzzCRKQvn0HnsK7f8wRcLp"
access_token = "328069252-DNksJDI286YcorHMRYDXMlNpo0nWFh4qwA4mds91"
access_token_secret = "hUSBB0N9xzbpJ6Y8o2BRYVCVf3YXcG6P71BUljKSjUlmO"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
ibrary(ROAuth)
ibrary(ROAuth)
install.packages("ROAuth")
ibrary(ROAuth)
library(ROAuth)
install.packages("base64enc","devtools","memoise","whisker","rstudioapi",
"git2r","withr","rjson","bit64","httr","httpuv")
install.packages("devtools")
api_key = "'yYf0zjyxgZqlu9j5MDOWq6okG'"
api_secret = "OQtmCCacdDMJPNDgxhOFNy69LJEhIzzCRKQvn0HnsK7f8wRcLp"
access_token = "328069252-DNksJDI286YcorHMRYDXMlNpo0nWFh4qwA4mds91"
access_token_secret = "hUSBB0N9xzbpJ6Y8o2BRYVCVf3YXcG6P71BUljKSjUlmO"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
install.package
install.packages("ROAuth")
library(ROAuth)
library(twitteR)
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
library(twitteR)
library(ROAuth)
api_key = "'yYf0zjyxgZqlu9j5MDOWq6okG'"
api_secret = "OQtmCCacdDMJPNDgxhOFNy69LJEhIzzCRKQvn0HnsK7f8wRcLp"
access_token = "328069252-DNksJDI286YcorHMRYDXMlNpo0nWFh4qwA4mds91"
access_token_secret = "hUSBB0N9xzbpJ6Y8o2BRYVCVf3YXcG6P71BUljKSjUlmO"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
install.packages("RCurl")
install.packages("RCurl")
library(RCurl)
#install.packages("twitterR")
library(twitteR)
#install.packages("ROAuth")
library(ROAuth)
#install.packages("RCurl")
library(RCurl)
api_key = "'sk3kl8hTNBfCe6LAftNj4HFfh'"
api_secret = "xIMy2JMKFIO39gDOQ2elG5YSmKDiGVSYOe4sJhepi5vtEQ9UQt"
access_token = "328069252-XO0bkZQQ9TpxJHsFc6JAmSiGKd485Atam0ziIZik"
access_token_secret = "DDsNjJDMkjF3tQ2awRuYOgyKDz9XHtJTR9ZJqCycXEOKh "
rent = searchTwitter("rent_in_ireland OR #rent_in_ireland OR accommodation_in_ireland OR
#accommodation_in_ireland OR housing_in_ireland OR #housing_in_ireland", n = 3000, lang = 'en')
api_key = "'sk3kl8hTNBfCe6LAftNj4HFfh'"
api_secret = "xIMy2JMKFIO39gDOQ2elG5YSmKDiGVSYOe4sJhepi5vtEQ9UQt"
access_token = "328069252-XO0bkZQQ9TpxJHsFc6JAmSiGKd485Atam0ziIZik"
access_token_secret = "DDsNjJDMkjF3tQ2awRuYOgyKDz9XHtJTR9ZJqCycXEOKh"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
api_key = "sk3kl8hTNBfCe6LAftNj4HFfh"
api_secret = "xIMy2JMKFIO39gDOQ2elG5YSmKDiGVSYOe4sJhepi5vtEQ9UQt"
access_token = "328069252-XO0bkZQQ9TpxJHsFc6JAmSiGKd485Atam0ziIZik"
access_token_secret = "DDsNjJDMkjF3tQ2awRuYOgyKDz9XHtJTR9ZJqCycXEOKh"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
rent = searchTwitter("rent_in_ireland OR #rent_in_ireland OR accommodation_in_ireland OR
#accommodation_in_ireland OR housing_in_ireland OR #housing_in_ireland", n = 3000, lang = 'en')
View(rent)
twListToDF(rent)
View(rent)
data.frame(rent)
data1 <- twListToDF(rent)
View(data1)
library(rvest)
boards.ie <- read_html("https://www.boards.ie/search/submit/?query=accommodation")
boards.ie <- read_html("https://www.boards.ie/search/submit/?sort=best&date_to=&date_from=&query=rent&page=1")
boards.ie %>%
html_node("title") %>%
html_text()
boards.ie %>%
html_nodes("p.result_snippet")%>%
html_text()
boards.ie %>%
html_nodes("span.date")%>%
html_text()
time <- boards.ie %>%
html_nodes("span.date")%>%
html_text()
forum <- boards.ie %>%
html_nodes("a.//www.boards.ie/vbulletin/forumdisplay.php?f=1208")%>%
html_text()
forum <- boards.ie %>%
html_nodes("a.php?f=1208")%>%
html_text()
boards.ie %>%
html_nodes("p.result_stats")%>%
html_text()
urls <- boards.ie %>%
html_node("//www.boards.ie/vbulletin/forumdisplay.php?f=1208") %>%
html_attr("href")
urls <- boards.ie %>%
html_node("a.//www.boards.ie/vbulletin/forumdisplay.php?f=1208") %>%
html_attr("href")
urls <- boards.ie %>%
html_node("a.php?f=1208") %>%
html_attr("href")
urls <- boards.ie %>%
html_node("a.result_stats") %>%
html_attr("href")
urls <- boards.ie %>%
html_node("a.result_snippet") %>%
html_attr("href")
urls
urls <- boards.ie %>%
html_node("a.result_stats") %>%
html_attr("href")
urls
boards.ie_webpage <- read_html("https://www.boards.ie/search/submit/?sort=best&date_to=&date_from=&query=rent&page=1")
boards.ie_webpage %>%
html_node("title") %>%
html_text()
boards.ie_webpage %>%
html_nodes("p.result_snippet")%>%
html_text()
urls <- boards.ie_webpage %>%
html_node("p.result_stats") %>%
html_attr("href")
urls
title
title <- boards.ie_webpage %>%
html_node("p.result_title") %>%
html_text()
title
title1 <- boards.ie_webpage %>%
html_node("p.result_wrapper") %>%
html_text()
title1
title1 <- boards.ie_webpage %>%
html_node("div.result_wrapper") %>%
html_text()
title1
test <- read_html("https://www.boards.ie/search/submit/?sort=best&date_to=&date_from=&query=accommodation&forum=38")
title <- test %>%
html_node("p.result_title") %>%
html_text()
title
library(rvest)
boards.ie_webpage <- read_html("https://www.boards.ie/search/submit/?sort=best&date_to=&date_from=&query=rent&page=1")
title <- boards.ie_webpage %>%
html_node("p.result_title") %>%
html_text()
title
title1 <- boards.ie_webpage %>%
html_nodes("p.result_wrapper") %>%
html_text()+
comment <- boards.ie_webpage %>%
html_nodes("p.result_snippet")%>%
html_text()
title1 <- boards.ie_webpage %>%
html_nodes("p.result_wrapper") %>%
html_text()
title1
title <- boards.ie_webpage %>%
html_node("title") %>%
html_text()
title
comment <- boards.ie_webpage %>%
html_nodes("p")%>%
html_text()
comment
comment <- boards.ie_webpage %>%
html_nodes("p.result_snippet")%>%
html_text()
comment
title <- boards.ie_webpage %>%
html_node("p.title") %>%
html_text()
title
title <- boards.ie_webpage %>%
html_node("p.result_title") %>%
html_text()
title
title <- boards.ie_webpage %>%
html_nodes("b") %>%
html_text()
title
title <- boards.ie_webpage %>%
html_nodes("a") %>%
html_text()
a
forum <- boards.ie_webpage %>%
html_nodes("a.//www.boards.ie/vbulletin/forumdisplay.php?f=1208") %>%
html_attr("href")
forum <- boards.ie_webpage %>%
html_nodes("a.//www.boards.ie/vbulletin/forumdisplay.php?f=1208") %>%
html_text()
forum <- boards.ie_webpage %>%
html_nodes("a.a") %>%
html_text()
forum
boards.ie_comments <- read_html("https://www.boards.ie/search/submit/?query=rent")
urls <- boards.ie_comments %>%
html_nodes("div.result_wrapper") %>%
html_attr("href")
urls
boards.ie_webpage %>%
html_nodes("p.result_stats")%>%
html_text()
urls <- boards.ie_comments %>%
html_nodes("div.result_wrapper") %>%
html_attr("href")
urls
#intsalling rvest package to enable data scraping
#install.packages("rvest")
#load library
library(rvest)
#reading the html file of boards.ie
boards.ie_webpage <- read_html("https://www.boards.ie/search/submit/?forum=38&sort=newest&date_to=&date_from=&query=rent")
#find no. of pages
retrieveLastPage <- function(html){
pages_data <- html %>%
html_nodes('.paginator_jump last') %>%
html_text()
}
pages_data
#find no. of pages
retrieveLastPage <- function(html){
pages_data <- html %>%
html_node('.paginator_jump last') %>%
html_text()
}
pages_data
#find no. of pages
retrieveLastPage <- function(boards.ie_webpage){
pages_data <- boards.ie_webpage %>%
html_node('.paginator_jump last') %>%
html_text()
}
pages_data
#find no. of pages
retrieveLastPage <- function(boards.ie_webpage){
pages_data <- boards.ie_webpage %>%
html_nodes('.paginator_jump last') %>%
html_text()
pages_data [(length(pages_data))] %>%
unname() %>%
as.numeric()
}
#find no. of pages
retrieveLastPage <- function(html){
pages_data <- html %>%
html_nodes('.paginator_jump last') %>%
html_text()
pages_data [(length(pages_data))] %>%
unname() %>%
as.numeric()
}
(a_page <- retrieveLastPage(boards.ie_webpage))
#find no. of pages
retrieveLastPage <- function(html){
pages_data <- html %>%
html_nodes('.paginator_jump') %>%
html_text()
pages_data [(length(pages_data))] %>%
unname() %>%
as.numeric()
}
(a_page <- retrieveLastPage(boards.ie_webpage))
#installed and loaded for effective data manipulation
install.packages("tidyverse")
)
library(tidyverse)
install.packages(stringr)
if (!('stringr' %in% installed.packages())) {
install.packages('stringr')
}
install.packages("stringr")
library(stringr)
#find no. of pages
retrieveLastPage <- function(html){
pages_data <- html %>%
html_nodes('.paginator_jump') %>%
html_text()
pages_data [(length(pages_data))] %>%
unname() %>%
as.numeric()
}
(a_page <- retrieveLastPage(boards.ie_webpage))
(a_page <- retrieveLastPage(boards.ie_webpage))
#find no. of pages
retrieveLastPage <- function(html){
pages_data <- html %>%
html_nodes("a.paginator_jump") %>%
html_text()
pages_data [(length(pages_data))] %>%
unname() %>%
as.numeric()
}
(a_page <- retrieveLastPage(boards.ie_webpage))
install.packages("purrr")
install.packages("purrr")
library(purrr)
boards_test <- read_html("https://www.boards.ie/search/submit/?forum=38&sort=newest&date_to=&date_from=&query=rent&page=%d")
boards_test <- "https://www.boards.ie/search/submit/?forum=38&sort=newest&date_to=&date_from=&query=rent&page=%d"
map_df(1:3, function(i){
page <- read_html(sprintf(boards_test, i))
data.frame(Title = html_text(html_nodes(page, ".result_title")),
Comment = html_text(html_nodes(page, ".result_snippet")),
Time = html_text(html_nodes(page, ".date"))
)
})
map_df(1:3, function(i){
page <- html(sprintf(boards_test, i))
data.frame(Title = html_text(html_nodes(page, ".result_title")),
Comment = html_text(html_nodes(page, ".result_snippet")),
Time = html_text(html_nodes(page, ".date"))
)
})
install.packages("xml2")
library(xml2)
#installing and loading rvest package to parse HTML/XML files
#install.packages("rvest")
library(rvest)
library(tidyverse)
library(stringr)
library(purrr)
boards_test <- "https://www.boards.ie/search/submit/?forum=38&sort=newest&date_to=&date_from=&query=rent&page=%d"
map_df(1:3, function(i){
page <- read_html(sprintf(boards_test, i))
data.frame(Title = html_text(html_nodes(page, ".result_title")),
Comment = html_text(html_nodes(page, ".result_snippet")),
Time = html_text(html_nodes(page, ".date"))
)
})
view(boards_test)
boards_data <- Data_frame
boards_data <- data.frame
View(boards.ie_webpage)
View(boards.ie_comments)
View(boards.ie)
view(boards_data)
map_df(1:3, function(i){
page <- read_html(sprintf(boards_test, i))
boards_data <- data.frame(Title = html_text(html_nodes(page, ".result_title")),
Comment = html_text(html_nodes(page, ".result_snippet")),
Time = html_text(html_nodes(page, ".date"))
)
})
view(board)
view(boards_data)
map_df(1:3, function(i){
page <- read_html(sprintf(boards_test, i))
data.frame(Title = html_text(html_nodes(page, ".result_title")),
Comment = html_text(html_nodes(page, ".result_snippet")),
Time = html_text(html_nodes(page, ".date"))
)
}) -> boards_data
View(boards_data)
citation()
library(tm)
boardsCorpus <- Corpus(VectorSource(boards_data))
inspect(boardsCorpus)
removeSpace <- content_transformer(function(x, pattern)
{return(gsub(pattern, "", x))})
Space <- content_transformer(function(x, pattern)
{return(gsub(pattern, " ", x))})
boards_data <- tm_map(boards_data, Space, "_")
boardsCorpus <- tm_map(boardsCorpus, Space, "_")
inspect(boardsCorpus)
boardsCorpus <- tm_map(boardsCorpus, Space, ":")
inspect(boardsCorpus$`1`)
boardsCorpus <- tm_map(boardsCorpus, Space, ",")
boardsCorpus <- tm_map(boardsCorpus, Space, "'")
boardsCorpus <- tm_map(boardsCorpus, Space, " -")
inspect(boardsCorpus)
boardsCorpus <- tm_map(boardsCorpus, Space, ".")
boardsCorpus <- tm_map(boardsCorpus, Space, ";")
#to remove punctuation marks
boardsCorpus <- tm_map(boardsCorpus, removePunctuation)
inspect(boardsCorpus)
View(boardsCorpus)
boardsCorpus <- tm_map(boardsCorpus, Space, "-")
#to transform all characters to lower case
boardsCorpus <- tm_map(boardsCorpus, content_transformer(tolower))
inspect(boardsCorpus)
inspect(boardsCorpus)
#For text mining
#install.packages("tm")
library(tm)
boardsCorpus <- Corpus(VectorSource(boards_data$Comment))
inspect(boardsCorpus)
Space <- content_transformer(function(x, pattern)
{return(gsub(pattern, " ", x))})
boardsCorpus <- tm_map(boardsCorpus, Space, "_")
boardsCorpus <- tm_map(boardsCorpus, Space, ":")
boardsCorpus <- tm_map(boardsCorpus, Space, ",")
boardsCorpus <- tm_map(boardsCorpus, Space, "'")
boardsCorpus <- tm_map(boardsCorpus, Space, " -")
boardsCorpus <- tm_map(boardsCorpus, Space, ".")
boardsCorpus <- tm_map(boardsCorpus, Space, ";")
inspect(boardsCorpus)
focus <- boards_data$Comment
focus <- data.frame(boards_data$Comment)
View(focus)
##Creat Corpus
boardsCorpus <- Corpus(VectorSource(focus))
Space <- content_transformer(function(x, pattern)
{return(gsub(pattern, " ", x))})
inspect(boardsCorpus)
focus <- data.frame(boards_data$Comment)
##Creat Corpus
boardsCorpus <- Corpus(VectorSource(focus))
writeLines(as.character(boardsCorpus[[2]]))
inspect(boardsCorpus)
#********Data Pre-processing**********
##Loading and installing required packages
#install.packages("stringr") #to remove whitespace characters from the string
library(stringr)
#install.packages("rJava")
#install.packages("RWeka")
#install.packages("qdap")
#install.packages("textir")
#install.packages("maptpx")
library(rJava)
library(RWeka)
library(qdap)
library(textir)
library(maptpx)
#for hypothesis testing
#install.packages("infer")
#ibrary(infer)
#For text mining
#install.packages("tm")
library(tm)
#install.packages("SnowballC")
library(SnowballC)
#install.packages("wordcloud")
library(wordcloud)
#focus <- data.frame(boards_data$Comment)
#write.table(focus, file = "boardsPosts.tsv", quote = FALSE, sep = '\t', row.names = FALSE, col.names = FALSE)
#write_tsv(focus, file = "boardsPosts2.tsv", sep = '\t', row.names = FALSE, col.names = FALSE)
#Posts <- read_tsv("boardsPosts.tsv")
setwd("C:/Users/hp/OneDrive - National College of Ireland/MSc. Research")
install.packages("rJava")
install.packages("RWeka")
#install.packages("qdap")
#install.packages("textir")
#install.packages("maptpx")
library(rJava)
library(RWeka)
#write.table(focus, file = "boardsPosts.tsv", quote = FALSE, sep = '\t', row.names = FALSE, col.names = FALSE)
#write_tsv(focus, file = "boardsPosts2.tsv", sep = '\t', row.names = FALSE, col.names = FALSE)
#Posts <- read_tsv("boardsPosts.tsv")
setwd("C:/Users/hp/OneDrive - National College of Ireland/MSc. Research")
focus <- read.csv("boardstest.csv", stringsAsFactors = FALSE)
View(focus)
focuscomment <- data.frame(focus$Comment)
View(focuscomment)
#install.packages("sentimentr") to enable sentiment analysis
library(sentimentr)
focus_comment <- get_sentences(focuscomment) #to get the right data structure
##Create Corpus
boardsCorpus <- Corpus(VectorSource(focuscomment$focus.Comment))
inspect(CleanCorpus[1:5])
#to transform all characters to lower case
CleanCorpus <- tm_map(boardsCorpus, content_transformer(tolower))
inspect(CleanCorpus[1:5])
#to remove punctuation marks
CleanCorpus <- tm_map(CleanCorpus, removePunctuation)
inspect(CleanCorpus[1:5])
inspect(CleanCorpus[40:45])
#To remove numbers
CleanCorpus <- tm_map(CleanCorpus, removeNumbers)
#to remove stopwords from standard stopword list
CleanCorpus <- tm_map(CleanCorpus, removeWords, stopwords("english"))
inspect(CleanCorpus[40:45])
#to remove whitespace
CleanCorpus <- tm_map(CleanCorpus, stripWhitespace)
inspect(CleanCorpus[40:45])
CleanCorpus_dtm <- TermDocumentMatrix(CleanCorpus)
CleanCorpus_m <- as.matrix(CleanCorpus_dtm)
dim(CleanCorpus_m)
str(focuscomment)
wordcloud(CleanCorpus)
#to remove stopwords from standard stopword list
CleanCorpus <- tm_map(CleanCorpus, removeWords, c(stopwords("english"), "rent"))
wordcloud(CleanCorpus)
wordcloud(CleanCorpus, random.order = FALSE)
wordcloud(CleanCorpus)
wordcloud(CleanCorpus, random.color = TRUE)
wordcloud(CleanCorpus, col = rainbow(7))
